{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scrapping Nigeria Rent and Sale Propety from PropertPro** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to extract property details\n",
    "def extract_property_details(soup):\n",
    "    properties = []\n",
    "    \n",
    "    # Find all property listings\n",
    "    listings = soup.find_all('div', class_='property-listing-grid')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Extract price\n",
    "            price = listing.find('div', class_='pl-price').find('h3').text.strip()\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            # Extract title\n",
    "            title = listing.find('div', class_='pl-title').text.strip()\n",
    "        except AttributeError:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract property ID\n",
    "            pid = listing.find('p').text.strip().replace(\"PID :\", \"\")\n",
    "        except AttributeError:\n",
    "            pid = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract number of beds and baths\n",
    "            details = listing.find('h6').text.strip()\n",
    "        except AttributeError:\n",
    "            details = 'N/A'\n",
    "        \n",
    "        properties.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'PID': pid,\n",
    "            'Details': details\n",
    "        })\n",
    "        \n",
    "    return properties\n",
    "\n",
    "# URL template (with page number placeholder)\n",
    "url_template = 'https://www.propertypro.ng/property-for-sale?page={}'\n",
    "\n",
    "# Number of pages to crawl (adjust based on the total pages)\n",
    "num_pages = 719\n",
    "\n",
    "# File to save progress\n",
    "output_file = 'propertypro_sale_listings.csv'\n",
    "last_page_file = 'last_page.txt'\n",
    "\n",
    "# List to store all scraped properties\n",
    "all_properties = []\n",
    "\n",
    "# Check if there's already a last saved page to resume from\n",
    "if os.path.exists(last_page_file):\n",
    "    with open(last_page_file, 'r') as f:\n",
    "        last_page = int(f.read().strip()) + 1\n",
    "else:\n",
    "    last_page = 1\n",
    "\n",
    "# Crawl through multiple pages, starting from the last saved page\n",
    "for page_num in range(last_page, num_pages + 1):\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    url = url_template.format(page_num)\n",
    "    \n",
    "    try:\n",
    "        # Send a GET request to fetch the HTML content\n",
    "        response = requests.get(url, timeout=10)  # Add timeout to handle slow responses\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract property details from the page\n",
    "            properties = extract_property_details(soup)\n",
    "            \n",
    "            # Add the scraped data to the list\n",
    "            all_properties.extend(properties)\n",
    "            \n",
    "            # Save progress every 20 pages\n",
    "            if page_num % 20 == 0:\n",
    "                df = pd.DataFrame(all_properties)\n",
    "                \n",
    "                if os.path.exists(output_file):\n",
    "                    # Append to the existing file\n",
    "                    df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    # Save as a new file\n",
    "                    df.to_csv(output_file, index=False)\n",
    "                \n",
    "                all_properties = []  # Clear list after saving\n",
    "                print(f\"Saved progress at page {page_num}.\")\n",
    "            \n",
    "            # Update last scraped page\n",
    "            with open(last_page_file, 'w') as f:\n",
    "                f.write(str(page_num))\n",
    "            \n",
    "            # Delay to avoid overloading the server\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {page_num}, status code: {response.status_code}\")\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error on page {page_num}: {e}\")\n",
    "        # Retry fetching the page after a short delay\n",
    "        time.sleep(5)\n",
    "        continue  # Skip to the next page\n",
    "\n",
    "# Final save of any remaining data\n",
    "if all_properties:\n",
    "    df = pd.DataFrame(all_properties)\n",
    "    df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "    print(f\"Final data saved to '{output_file}'.\")\n",
    "\n",
    "print(\"Scraping complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Function to extract property details for rent listings\n",
    "def extract_rental_details(soup):\n",
    "    properties = []\n",
    "    \n",
    "    # Find all property listings for rent\n",
    "    listings = soup.find_all('div', class_='property-listing-grid')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Extract price\n",
    "            price = listing.find('div', class_='pl-price').find('h3').text.strip()\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            # Extract title\n",
    "            title = listing.find('div', class_='pl-title').text.strip()\n",
    "        except AttributeError:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract property ID\n",
    "            pid = listing.find('p').text.strip().replace(\"PID :\", \"\")\n",
    "        except AttributeError:\n",
    "            pid = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract number of beds and baths\n",
    "            details = listing.find('h6').text.strip()\n",
    "        except AttributeError:\n",
    "            details = 'N/A'\n",
    "        \n",
    "        properties.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'PID': pid,\n",
    "            'Details': details\n",
    "        })\n",
    "        \n",
    "    return properties\n",
    "\n",
    "# URL template for houses for rent (with page number placeholder)\n",
    "url_template = 'https://www.propertypro.ng/property-for-rent?page={}'\n",
    "\n",
    "# Number of pages to crawl (you can adjust this)\n",
    "num_pages = 293\n",
    "\n",
    "# List to store all scraped properties\n",
    "all_rental_properties = []\n",
    "\n",
    "# Crawl through multiple pages for rent listings\n",
    "for page_num in range(1, num_pages + 1):\n",
    "    print(f\"Scraping page {page_num} for rent listings...\")\n",
    "    url = url_template.format(page_num)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        rental_properties = extract_rental_details(soup)\n",
    "        all_rental_properties.extend(rental_properties)\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_num}, status code: {response.status_code}\")\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df_rentals = pd.DataFrame(all_rental_properties)\n",
    "\n",
    "# Save the data into a CSV file\n",
    "df_rentals.to_csv('propertypro_rent_listings.csv', index=False)\n",
    "print(\"Data saved to 'propertypro_rent_listings.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scrapping Rent and Sale Properties for Kenya**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Function to extract property details\n",
    "def extract_property_details(soup):\n",
    "    properties = []\n",
    "    \n",
    "    # Find all property listings\n",
    "    listings = soup.find_all('div', class_='sc_panelWrapper')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Extract price\n",
    "            price = listing.find('div', class_='p24_price').text.strip()\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            # Extract title\n",
    "            title = listing.find('div', class_='p24_regularTile').text.strip()\n",
    "        except AttributeError:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract property details (e.g., bedrooms, bathrooms)\n",
    "            details = listing.find('span', class_='js_listingTileImageHolder').text.strip()\n",
    "        except AttributeError:\n",
    "            details = 'N/A'\n",
    "        \n",
    "        properties.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Details': details\n",
    "        })\n",
    "        \n",
    "    return properties\n",
    "\n",
    "# Base URL template (with placeholders for province name and ID)\n",
    "base_url = 'https://www.property24.co.ke/property-for-sale-in-{}-p{}?Page={}'\n",
    "\n",
    "# List of provinces with their associated IDs\n",
    "provinces = {\n",
    "    'mombasa': 93,\n",
    "    'kwale': 85,\n",
    "    'kilifi': 80,\n",
    "    'tana river': 105,\n",
    "    'lamu': 87,\n",
    "    'taita–taveta': 104,\n",
    "    'garissa': 73,\n",
    "    'wajir': 111,\n",
    "    'mandera': 89,\n",
    "    'marsabit': 90,\n",
    "    'isiolo': 75,\n",
    "    'meru': 91,\n",
    "    'tharaka-nithi': 106,\n",
    "    'embu': 72,\n",
    "    'kitui': 84,\n",
    "    'machakos': 66,\n",
    "    'makueni': 88,\n",
    "    'nyandarua': 100,\n",
    "    'nyeri': 101,\n",
    "    'kirinyaga': 81,\n",
    "    'muranga': 94,\n",
    "    'kiambu': 79,\n",
    "    'turkana': 108,\n",
    "    'west pokot': 112,\n",
    "    'samburu': 102,\n",
    "    'trans-nzoia': 107,\n",
    "    'uasin gishu': 109,\n",
    "    'elgeyo-marakwet': 71,\n",
    "    'nandi': 97,\n",
    "    'baringo': 67,\n",
    "    'laikipia': 86,\n",
    "    'nakuru': 96,\n",
    "    'narok': 98,\n",
    "    'kajiado': 76,\n",
    "    'kericho': 78,\n",
    "    'bomet': 68,\n",
    "    'kakamega': 77,\n",
    "    'vihiga': 110,\n",
    "    'bungoma': 69,\n",
    "    'busia': 70,\n",
    "    'siaya': 103,\n",
    "    'kisumu': 83,\n",
    "    'homa bay': 74,\n",
    "    'migori': 92,\n",
    "    'kisii': 82,\n",
    "    'nyamira': 99,\n",
    "    'nairobi': 95\n",
    "    # Add more provinces and their IDs here...\n",
    "}\n",
    "\n",
    "# File to save progress and track last page scraped for each province\n",
    "progress_file = 'scraping_progress.json'\n",
    "\n",
    "# Load progress if it exists\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as file:\n",
    "        progress = json.load(file)\n",
    "else:\n",
    "    progress = {province: 1 for province in provinces}  # Start from page 1 for all provinces\n",
    "\n",
    "# Placeholder for all scraped properties\n",
    "all_properties = []\n",
    "\n",
    "# Function to scrape a specific province\n",
    "def scrape_province(province, province_id):\n",
    "    page_num = progress.get(province, 1)  # Start from the last saved page\n",
    "    while True:\n",
    "        print(f\"Scraping {province}, Page {page_num}...\")\n",
    "        \n",
    "        # Construct the URL with the province and page number\n",
    "        url = base_url.format(province, province_id, page_num)\n",
    "        \n",
    "        # Request the page\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {url}, status code: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        # Parse the content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract property details\n",
    "        properties = extract_property_details(soup)\n",
    "        \n",
    "        if not properties:\n",
    "            print(f\"No more listings found for {province}. Stopping at page {page_num}.\")\n",
    "            break\n",
    "        \n",
    "        # Add properties to the global list\n",
    "        all_properties.extend(properties)\n",
    "        \n",
    "        # Save progress after each page\n",
    "        progress[province] = page_num\n",
    "        with open(progress_file, 'w') as file:\n",
    "            json.dump(progress, file)\n",
    "        \n",
    "        # Save data incrementally to avoid data loss\n",
    "        pd.DataFrame(all_properties).to_csv('property24_kenya_listings.csv', index=False)\n",
    "        \n",
    "        # Check if a 'next' page link exists\n",
    "        next_button = soup.find('li', class_='pagelink')\n",
    "        if not next_button:\n",
    "            print(f\"Finished scraping {province} after {page_num} pages.\")\n",
    "            break\n",
    "        \n",
    "        # Delay between requests to avoid overloading the server\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Increment page number\n",
    "        page_num += 1\n",
    "\n",
    "# Loop through all provinces\n",
    "for province, province_id in provinces.items():\n",
    "    scrape_province(province, province_id)\n",
    "\n",
    "print(\"Scraping complete. Data saved to 'property24_kenya_listings.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping mombasa, Page 1...\n",
      "No more listings found for mombasa. Stopping at page 1.\n",
      "Scraping kwale, Page 1...\n",
      "No more listings found for kwale. Stopping at page 1.\n",
      "Scraping kilifi, Page 1...\n",
      "No more listings found for kilifi. Stopping at page 1.\n",
      "Scraping tana river, Page 1...\n",
      "No more listings found for tana river. Stopping at page 1.\n",
      "Scraping lamu, Page 1...\n",
      "No more listings found for lamu. Stopping at page 1.\n",
      "Scraping taita–taveta, Page 1...\n",
      "No more listings found for taita–taveta. Stopping at page 1.\n",
      "Scraping garissa, Page 1...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 174\u001b[0m\n\u001b[1;32m    172\u001b[0m         scrape_province(province, province_id, max_pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m         \u001b[43mscrape_province\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprovince\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovince_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping complete. Data saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproperty24_kenya_rent_listings.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 123\u001b[0m, in \u001b[0;36mscrape_province\u001b[0;34m(province, province_id, max_pages)\u001b[0m\n\u001b[1;32m    120\u001b[0m url \u001b[38;5;241m=\u001b[39m base_url\u001b[38;5;241m.\u001b[39mformat(formatted_province, province_id, page_num)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Request the page\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Check if the request was successful\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/urllib3/connection.py:615\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    614\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    617\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/urllib3/connection.py:196\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Function to extract property details\n",
    "def extract_property_details(soup):\n",
    "    properties = []\n",
    "    \n",
    "    # Find all property listings\n",
    "    listings = soup.find_all('div', class_='SootheAffine js_listingTile')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Extract price\n",
    "            price = listing.find('span', class_='p24_price').text.strip()\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            # Extract title\n",
    "            title = listing.find('span', class_='p24_propertyTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract location\n",
    "            location = listing.find('span', class_='p24_location').text.strip()\n",
    "        except AttributeError:\n",
    "            location = 'N/A'\n",
    "        \n",
    "        properties.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Location': location\n",
    "        })\n",
    "        \n",
    "    return properties\n",
    "\n",
    "# Base URL template (with placeholders for province name and ID)\n",
    "base_url = 'https://www.property24.co.ke/property-to-rent-in-{}-p{}?Page={}'\n",
    "\n",
    "# List of provinces with their associated IDs\n",
    "provinces = {\n",
    "    'mombasa': 93,\n",
    "    'kwale': 85,\n",
    "    'kilifi': 80,\n",
    "    'tana river': 105,\n",
    "    'lamu': 87,\n",
    "    'taita–taveta': 104,\n",
    "    'garissa': 73,\n",
    "    'wajir': 111,\n",
    "    'mandera': 89,\n",
    "    'marsabit': 90,\n",
    "    'isiolo': 75,\n",
    "    'meru': 91,\n",
    "    'tharaka-nithi': 106,\n",
    "    'embu': 72,\n",
    "    'kitui': 84,\n",
    "    'machakos': 66,\n",
    "    'makueni': 88,\n",
    "    'nyandarua': 100,\n",
    "    'nyeri': 101,\n",
    "    'kirinyaga': 81,\n",
    "    'muranga': 94,\n",
    "    'kiambu': 79,\n",
    "    'turkana': 108,\n",
    "    'west pokot': 112,\n",
    "    'samburu': 102,\n",
    "    'trans-nzoia': 107,\n",
    "    'uasin gishu': 109,\n",
    "    'elgeyo-marakwet': 71,\n",
    "    'nandi': 97,\n",
    "    'baringo': 67,\n",
    "    'laikipia': 86,\n",
    "    'nakuru': 96,\n",
    "    'narok': 98,\n",
    "    'kajiado': 76,\n",
    "    'kericho': 78,\n",
    "    'bomet': 68,\n",
    "    'kakamega': 77,\n",
    "    'vihiga': 110,\n",
    "    'bungoma': 69,\n",
    "    'busia': 70,\n",
    "    'siaya': 103,\n",
    "    'kisumu': 83,\n",
    "    'homa bay': 74,\n",
    "    'migori': 92,\n",
    "    'kisii': 82,\n",
    "    'nyamira': 99,\n",
    "    'nairobi': 95,\n",
    "    # Add more provinces and their IDs here...\n",
    "}\n",
    "\n",
    "# File to save progress and track last page scraped for each province\n",
    "progress_file = 'scraping_progress.json'\n",
    "\n",
    "# Load progress if it exists\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as file:\n",
    "        progress = json.load(file)\n",
    "else:\n",
    "    progress = {province: 1 for province in provinces}  # Start from page 1 for all provinces\n",
    "\n",
    "# Placeholder for all scraped properties\n",
    "all_properties = []\n",
    "\n",
    "# Function to scrape a specific province\n",
    "def scrape_province(province, province_id, max_pages=None):\n",
    "    page_num = progress.get(province, 1)  # Start from the last saved page\n",
    "    while True:\n",
    "        print(f\"Scraping {province}, Page {page_num}...\")\n",
    "        \n",
    "        # Properly format province names with hyphens instead of spaces\n",
    "        formatted_province = province.replace(' ', '-').lower()\n",
    "        \n",
    "        # Construct the URL with the province and page number\n",
    "        url = base_url.format(formatted_province, province_id, page_num)\n",
    "        \n",
    "        # Request the page\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {url}, status code: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        # Parse the content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract property details\n",
    "        properties = extract_property_details(soup)\n",
    "        \n",
    "        if not properties:\n",
    "            print(f\"No more listings found for {province}. Stopping at page {page_num}.\")\n",
    "            break\n",
    "        \n",
    "        # Add properties to the global list\n",
    "        all_properties.extend(properties)\n",
    "        \n",
    "        # Save progress after each page\n",
    "        progress[province] = page_num\n",
    "        with open(progress_file, 'w') as file:\n",
    "            json.dump(progress, file)\n",
    "        \n",
    "        # Save data incrementally to avoid data loss\n",
    "        pd.DataFrame(all_properties).to_csv('property24_kenya_rent_listings.csv', index=False)\n",
    "        \n",
    "        # Check if a 'next' page link exists\n",
    "        next_button = soup.find('li', class_='pagelink')\n",
    "        if not next_button:\n",
    "            print(f\"Finished scraping {province} after {page_num} pages.\")\n",
    "            break\n",
    "        \n",
    "        # Delay between requests to avoid overloading the server\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Stop if the max page limit is reached (for Nairobi or other large provinces)\n",
    "        if max_pages and page_num >= max_pages:\n",
    "            print(f\"Reached page limit of {max_pages} for {province}.\")\n",
    "            break\n",
    "        \n",
    "        # Increment page number\n",
    "        page_num += 1\n",
    "\n",
    "# Loop through all provinces\n",
    "for province, province_id in provinces.items():\n",
    "    # Limit Nairobi to 1000 pages, others to default behavior\n",
    "    if province == 'nairobi':\n",
    "        scrape_province(province, province_id, max_pages=1000)\n",
    "    else:\n",
    "        scrape_province(province, province_id)\n",
    "\n",
    "print(\"Scraping complete. Data saved to 'property24_kenya_rent_listings.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
