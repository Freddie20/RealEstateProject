{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scrapping Nigeria Rent and Sale Propety from PropertPro** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to extract property details\n",
    "def extract_property_details(soup):\n",
    "    properties = []\n",
    "    \n",
    "    # Find all property listings\n",
    "    listings = soup.find_all('div', class_='property-listing-grid')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Extract price\n",
    "            price = listing.find('div', class_='pl-price').find('h3').text.strip()\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            # Extract title\n",
    "            title = listing.find('div', class_='pl-title').text.strip()\n",
    "        except AttributeError:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract property ID\n",
    "            pid = listing.find('p').text.strip().replace(\"PID :\", \"\")\n",
    "        except AttributeError:\n",
    "            pid = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract number of beds and baths\n",
    "            details = listing.find('h6').text.strip()\n",
    "        except AttributeError:\n",
    "            details = 'N/A'\n",
    "        \n",
    "        properties.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'PID': pid,\n",
    "            'Details': details\n",
    "        })\n",
    "        \n",
    "    return properties\n",
    "\n",
    "# URL template (with page number placeholder)\n",
    "url_template = 'https://www.propertypro.ng/property-for-sale?page={}'\n",
    "\n",
    "# Number of pages to crawl (adjust based on the total pages)\n",
    "num_pages = 719\n",
    "\n",
    "# File to save progress\n",
    "output_file = 'propertypro_sale_listings.csv'\n",
    "last_page_file = 'last_page.txt'\n",
    "\n",
    "# List to store all scraped properties\n",
    "all_properties = []\n",
    "\n",
    "# Check if there's already a last saved page to resume from\n",
    "if os.path.exists(last_page_file):\n",
    "    with open(last_page_file, 'r') as f:\n",
    "        last_page = int(f.read().strip()) + 1\n",
    "else:\n",
    "    last_page = 1\n",
    "\n",
    "# Crawl through multiple pages, starting from the last saved page\n",
    "for page_num in range(last_page, num_pages + 1):\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    url = url_template.format(page_num)\n",
    "    \n",
    "    try:\n",
    "        # Send a GET request to fetch the HTML content\n",
    "        response = requests.get(url, timeout=10)  # Add timeout to handle slow responses\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract property details from the page\n",
    "            properties = extract_property_details(soup)\n",
    "            \n",
    "            # Add the scraped data to the list\n",
    "            all_properties.extend(properties)\n",
    "            \n",
    "            # Save progress every 20 pages\n",
    "            if page_num % 20 == 0:\n",
    "                df = pd.DataFrame(all_properties)\n",
    "                \n",
    "                if os.path.exists(output_file):\n",
    "                    # Append to the existing file\n",
    "                    df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    # Save as a new file\n",
    "                    df.to_csv(output_file, index=False)\n",
    "                \n",
    "                all_properties = []  # Clear list after saving\n",
    "                print(f\"Saved progress at page {page_num}.\")\n",
    "            \n",
    "            # Update last scraped page\n",
    "            with open(last_page_file, 'w') as f:\n",
    "                f.write(str(page_num))\n",
    "            \n",
    "            # Delay to avoid overloading the server\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {page_num}, status code: {response.status_code}\")\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error on page {page_num}: {e}\")\n",
    "        # Retry fetching the page after a short delay\n",
    "        time.sleep(5)\n",
    "        continue  # Skip to the next page\n",
    "\n",
    "# Final save of any remaining data\n",
    "if all_properties:\n",
    "    df = pd.DataFrame(all_properties)\n",
    "    df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "    print(f\"Final data saved to '{output_file}'.\")\n",
    "\n",
    "print(\"Scraping complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Function to extract property details for rent listings\n",
    "def extract_rental_details(soup):\n",
    "    properties = []\n",
    "    \n",
    "    # Find all property listings for rent\n",
    "    listings = soup.find_all('div', class_='property-listing-grid')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Extract price\n",
    "            price = listing.find('div', class_='pl-price').find('h3').text.strip()\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            # Extract title\n",
    "            title = listing.find('div', class_='pl-title').text.strip()\n",
    "        except AttributeError:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract property ID\n",
    "            pid = listing.find('p').text.strip().replace(\"PID :\", \"\")\n",
    "        except AttributeError:\n",
    "            pid = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract number of beds and baths\n",
    "            details = listing.find('h6').text.strip()\n",
    "        except AttributeError:\n",
    "            details = 'N/A'\n",
    "        \n",
    "        properties.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'PID': pid,\n",
    "            'Details': details\n",
    "        })\n",
    "        \n",
    "    return properties\n",
    "\n",
    "# URL template for houses for rent (with page number placeholder)\n",
    "url_template = 'https://www.propertypro.ng/property-for-rent?page={}'\n",
    "\n",
    "# Number of pages to crawl (you can adjust this)\n",
    "num_pages = 293\n",
    "\n",
    "# List to store all scraped properties\n",
    "all_rental_properties = []\n",
    "\n",
    "# Crawl through multiple pages for rent listings\n",
    "for page_num in range(1, num_pages + 1):\n",
    "    print(f\"Scraping page {page_num} for rent listings...\")\n",
    "    url = url_template.format(page_num)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        rental_properties = extract_rental_details(soup)\n",
    "        all_rental_properties.extend(rental_properties)\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_num}, status code: {response.status_code}\")\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df_rentals = pd.DataFrame(all_rental_properties)\n",
    "\n",
    "# Save the data into a CSV file\n",
    "df_rentals.to_csv('propertypro_rent_listings.csv', index=False)\n",
    "print(\"Data saved to 'propertypro_rent_listings.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scrapping Rent and Sale Properties for Kenya**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Function to extract property details\n",
    "def extract_property_details(soup):\n",
    "    properties = []\n",
    "    \n",
    "    # Find all property listings\n",
    "    listings = soup.find_all('div', class_='sc_panelWrapper')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Extract price\n",
    "            price = listing.find('div', class_='p24_price').text.strip()\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            # Extract title\n",
    "            title = listing.find('div', class_='p24_regularTile').text.strip()\n",
    "        except AttributeError:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract property details (e.g., bedrooms, bathrooms)\n",
    "            details = listing.find('span', class_='js_listingTileImageHolder').text.strip()\n",
    "        except AttributeError:\n",
    "            details = 'N/A'\n",
    "        \n",
    "        properties.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Details': details\n",
    "        })\n",
    "        \n",
    "    return properties\n",
    "\n",
    "# Base URL template (with placeholders for province name and ID)\n",
    "base_url = 'https://www.property24.co.ke/property-for-sale-in-{}-p{}?Page={}'\n",
    "\n",
    "# List of provinces with their associated IDs\n",
    "provinces = {\n",
    "    'mombasa': 93,\n",
    "    'kwale': 85,\n",
    "    'kilifi': 80,\n",
    "    'tana river': 105,\n",
    "    'lamu': 87,\n",
    "    'taita–taveta': 104,\n",
    "    'garissa': 73,\n",
    "    'wajir': 111,\n",
    "    'mandera': 89,\n",
    "    'marsabit': 90,\n",
    "    'isiolo': 75,\n",
    "    'meru': 91,\n",
    "    'tharaka-nithi': 106,\n",
    "    'embu': 72,\n",
    "    'kitui': 84,\n",
    "    'machakos': 66,\n",
    "    'makueni': 88,\n",
    "    'nyandarua': 100,\n",
    "    'nyeri': 101,\n",
    "    'kirinyaga': 81,\n",
    "    'muranga': 94,\n",
    "    'kiambu': 79,\n",
    "    'turkana': 108,\n",
    "    'west pokot': 112,\n",
    "    'samburu': 102,\n",
    "    'trans-nzoia': 107,\n",
    "    'uasin gishu': 109,\n",
    "    'elgeyo-marakwet': 71,\n",
    "    'nandi': 97,\n",
    "    'baringo': 67,\n",
    "    'laikipia': 86,\n",
    "    'nakuru': 96,\n",
    "    'narok': 98,\n",
    "    'kajiado': 76,\n",
    "    'kericho': 78,\n",
    "    'bomet': 68,\n",
    "    'kakamega': 77,\n",
    "    'vihiga': 110,\n",
    "    'bungoma': 69,\n",
    "    'busia': 70,\n",
    "    'siaya': 103,\n",
    "    'kisumu': 83,\n",
    "    'homa bay': 74,\n",
    "    'migori': 92,\n",
    "    'kisii': 82,\n",
    "    'nyamira': 99,\n",
    "    'nairobi': 95\n",
    "    # Add more provinces and their IDs here...\n",
    "}\n",
    "\n",
    "# File to save progress and track last page scraped for each province\n",
    "progress_file = 'scraping_progress.json'\n",
    "\n",
    "# Load progress if it exists\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as file:\n",
    "        progress = json.load(file)\n",
    "else:\n",
    "    progress = {province: 1 for province in provinces}  # Start from page 1 for all provinces\n",
    "\n",
    "# Placeholder for all scraped properties\n",
    "all_properties = []\n",
    "\n",
    "# Function to scrape a specific province\n",
    "def scrape_province(province, province_id):\n",
    "    page_num = progress.get(province, 1)  # Start from the last saved page\n",
    "    while True:\n",
    "        print(f\"Scraping {province}, Page {page_num}...\")\n",
    "        \n",
    "        # Construct the URL with the province and page number\n",
    "        url = base_url.format(province, province_id, page_num)\n",
    "        \n",
    "        # Request the page\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {url}, status code: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        # Parse the content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract property details\n",
    "        properties = extract_property_details(soup)\n",
    "        \n",
    "        if not properties:\n",
    "            print(f\"No more listings found for {province}. Stopping at page {page_num}.\")\n",
    "            break\n",
    "        \n",
    "        # Add properties to the global list\n",
    "        all_properties.extend(properties)\n",
    "        \n",
    "        # Save progress after each page\n",
    "        progress[province] = page_num\n",
    "        with open(progress_file, 'w') as file:\n",
    "            json.dump(progress, file)\n",
    "        \n",
    "        # Save data incrementally to avoid data loss\n",
    "        pd.DataFrame(all_properties).to_csv('property24_kenya_listings.csv', index=False)\n",
    "        \n",
    "        # Check if a 'next' page link exists\n",
    "        next_button = soup.find('li', class_='pagelink')\n",
    "        if not next_button:\n",
    "            print(f\"Finished scraping {province} after {page_num} pages.\")\n",
    "            break\n",
    "        \n",
    "        # Delay between requests to avoid overloading the server\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Increment page number\n",
    "        page_num += 1\n",
    "\n",
    "# Loop through all provinces\n",
    "for province, province_id in provinces.items():\n",
    "    scrape_province(province, province_id)\n",
    "\n",
    "print(\"Scraping complete. Data saved to 'property24_kenya_listings.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Function to extract property details\n",
    "def extract_property_details(soup):\n",
    "    properties = []\n",
    "    \n",
    "    # Find all property listings\n",
    "    listings = soup.find_all('div', class_='sc_panelWrapper')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Extract price\n",
    "            price = listing.find('span', class_='p24_price').text.strip()\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            # Extract title\n",
    "            title = listing.find('span', class_='p24_propertyTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract property details (e.g., location)\n",
    "            details = listing.find('span', class_='p24_location').text.strip()\n",
    "        except AttributeError:\n",
    "            details = 'N/A'\n",
    "        \n",
    "        properties.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Details': details\n",
    "        })\n",
    "        \n",
    "    return properties\n",
    "\n",
    "# Base URL template (with placeholders for province name and ID)\n",
    "base_url = 'https://www.property24.co.ke/property-to-rent-in-{}-p{}?Page={}'\n",
    "\n",
    "# List of provinces with their associated IDs\n",
    "provinces = {\n",
    "    'mombasa': 93,\n",
    "    'kwale': 85,\n",
    "    'kilifi': 80,\n",
    "    'tana river': 105,\n",
    "    'lamu': 87,\n",
    "    'taita–taveta': 104,\n",
    "    'garissa': 73,\n",
    "    'wajir': 111,\n",
    "    'mandera': 89,\n",
    "    'marsabit': 90,\n",
    "    'isiolo': 75,\n",
    "    'meru': 91,\n",
    "    'tharaka-nithi': 106,\n",
    "    'embu': 72,\n",
    "    'kitui': 84,\n",
    "    'machakos': 66,\n",
    "    'makueni': 88,\n",
    "    'nyandarua': 100,\n",
    "    'nyeri': 101,\n",
    "    'kirinyaga': 81,\n",
    "    'muranga': 94,\n",
    "    'kiambu': 79,\n",
    "    'turkana': 108,\n",
    "    'west pokot': 112,\n",
    "    'samburu': 102,\n",
    "    'trans-nzoia': 107,\n",
    "    'uasin gishu': 109,\n",
    "    'elgeyo-marakwet': 71,\n",
    "    'nandi': 97,\n",
    "    'baringo': 67,\n",
    "    'laikipia': 86,\n",
    "    'nakuru': 96,\n",
    "    'narok': 98,\n",
    "    'kajiado': 76,\n",
    "    'kericho': 78,\n",
    "    'bomet': 68,\n",
    "    'kakamega': 77,\n",
    "    'vihiga': 110,\n",
    "    'bungoma': 69,\n",
    "    'busia': 70,\n",
    "    'siaya': 103,\n",
    "    'kisumu': 83,\n",
    "    'homa bay': 74,\n",
    "    'migori': 92,\n",
    "    'kisii': 82,\n",
    "    'nyamira': 99,\n",
    "    'nairobi': 95, # Limiting Nairobi to 1000 pages later in the loop\n",
    "}\n",
    "\n",
    "# File to save progress and track last page scraped for each province\n",
    "progress_file = 'scraping_progress_rent.json'\n",
    "\n",
    "# Load progress if it exists\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as file:\n",
    "        progress = json.load(file)\n",
    "else:\n",
    "    progress = {province: 1 for province in provinces}  # Start from page 1 for all provinces\n",
    "\n",
    "# Placeholder for all scraped properties\n",
    "all_properties = []\n",
    "\n",
    "# Function to scrape a specific province\n",
    "def scrape_province(province, province_id):\n",
    "    page_num = progress.get(province, 1)  # Start from the last saved page\n",
    "    max_pages = 1000 if province == 'nairobi' else 9999  # Limit to 1000 pages for Nairobi\n",
    "    \n",
    "    while page_num <= max_pages:\n",
    "        print(f\"Scraping {province}, Page {page_num}...\")\n",
    "        \n",
    "        # Construct the URL with the province and page number\n",
    "        url = base_url.format(province, province_id, page_num)\n",
    "        \n",
    "        # Request the page\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {url}, status code: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        # Parse the content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract property details\n",
    "        properties = extract_property_details(soup)\n",
    "        \n",
    "        if not properties:\n",
    "            print(f\"No more listings found for {province}. Stopping at page {page_num}.\")\n",
    "            break\n",
    "        \n",
    "        # Add properties to the global list\n",
    "        all_properties.extend(properties)\n",
    "        \n",
    "        # Save progress after each page\n",
    "        progress[province] = page_num\n",
    "        with open(progress_file, 'w') as file:\n",
    "            json.dump(progress, file)\n",
    "        \n",
    "        # Save data incrementally to avoid data loss\n",
    "        pd.DataFrame(all_properties).to_csv('property24_kenya_rent_listings.csv', index=False)\n",
    "        \n",
    "        # Check if a 'next' page link exists\n",
    "        next_button = soup.find('li', class_='pagelink')\n",
    "        if not next_button:\n",
    "            print(f\"Finished scraping {province} after {page_num} pages.\")\n",
    "            break\n",
    "        \n",
    "        # Delay between requests to avoid overloading the server\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Increment page number\n",
    "        page_num += 1\n",
    "\n",
    "# Loop through all provinces\n",
    "for province, province_id in provinces.items():\n",
    "    scrape_province(province, province_id)\n",
    "\n",
    "print(\"Scraping complete. Data saved to 'property24_kenya_rent_listings.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Function to extract property details\n",
    "def extract_property_details(soup):\n",
    "    properties = []\n",
    "    \n",
    "    # Find all property listings\n",
    "    listings = soup.find_all('div', class_='sc_panelWrapper')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Extract price\n",
    "            price = listing.find('span', class_='p24_price').text.strip()\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            # Extract title\n",
    "            title = listing.find('span', class_='p24_propertyTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract property details (e.g., location)\n",
    "            details = listing.find('span', class_='p24_location').text.strip()\n",
    "        except AttributeError:\n",
    "            details = 'N/A'\n",
    "        \n",
    "        properties.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Details': details\n",
    "        })\n",
    "        \n",
    "    return properties\n",
    "\n",
    "# Base URL template (with placeholders for province name and ID)\n",
    "base_url = 'https://www.property24.co.ke/property-for-sale-in-{}-p{}?Page={}'\n",
    "\n",
    "# List of provinces with their associated IDs\n",
    "provinces = {\n",
    "    'mombasa': 93,\n",
    "    'nairobi': 95, # Limiting Nairobi to 1000 pages later in the loop\n",
    "}\n",
    "\n",
    "# File to save progress and track last page scraped for each province\n",
    "progress_file = 'scraping_progress_sale.json'\n",
    "\n",
    "# Load progress if it exists\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as file:\n",
    "        progress = json.load(file)\n",
    "else:\n",
    "    progress = {province: 1 for province in provinces}  # Start from page 1 for all provinces\n",
    "\n",
    "# Placeholder for all scraped properties\n",
    "all_properties = []\n",
    "\n",
    "# Function to scrape a specific province\n",
    "def scrape_province(province, province_id):\n",
    "    page_num = progress.get(province, 1)  # Start from the last saved page\n",
    "    max_pages = 1000 if province == 'nairobi' else 9999  # Limit to 1000 pages for Nairobi\n",
    "    \n",
    "    while page_num <= max_pages:\n",
    "        print(f\"Scraping {province}, Page {page_num}...\")\n",
    "        \n",
    "        # Construct the URL with the province and page number\n",
    "        url = base_url.format(province, province_id, page_num)\n",
    "        \n",
    "        # Request the page\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {url}, status code: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        # Parse the content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract property details\n",
    "        properties = extract_property_details(soup)\n",
    "        \n",
    "        if not properties:\n",
    "            print(f\"No more listings found for {province}. Stopping at page {page_num}.\")\n",
    "            break\n",
    "        \n",
    "        # Add properties to the global list\n",
    "        all_properties.extend(properties)\n",
    "        \n",
    "        # Save progress after each page\n",
    "        progress[province] = page_num\n",
    "        with open(progress_file, 'w') as file:\n",
    "            json.dump(progress, file)\n",
    "        \n",
    "        # Save data incrementally to avoid data loss\n",
    "        pd.DataFrame(all_properties).to_csv('property24_kenya_sale_listings.csv', index=False)\n",
    "        \n",
    "        # Check if a 'next' page link exists\n",
    "        next_button = soup.find('li', class_='pagelink')\n",
    "        if not next_button:\n",
    "            print(f\"Finished scraping {province} after {page_num} pages.\")\n",
    "            break\n",
    "        \n",
    "        # Delay between requests to avoid overloading the server\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Increment page number\n",
    "        page_num += 1\n",
    "\n",
    "# Loop through all provinces\n",
    "for province, province_id in provinces.items():\n",
    "    scrape_province(province, province_id)\n",
    "\n",
    "print(\"Scraping complete. Data saved to 'property24_kenya_sale_listings.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DR CONGO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping complete. Data saved to 'imcongo_rent_listings.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to extract property details\n",
    "def extract_property_details(soup):\n",
    "    properties = []\n",
    "    \n",
    "    # Find all property listings\n",
    "    listings = soup.find_all('div', class_='ligne_bien')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Extract price (nested inside <font> within <div class=\"price\">)\n",
    "            price = listing.find('div', class_='price').find('font').text.strip()\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            # Extract title (from <a> tag)\n",
    "            anchor = listing.find('a')\n",
    "            title = anchor['title'].strip()\n",
    "            url = anchor['href']\n",
    "        except AttributeError:\n",
    "            title = 'N/A'\n",
    "            url = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract room details (from <font> tag with room info)\n",
    "            rooms = listing.find_all('font', style=\"vertical-align: inherit;\")[1].text.strip()\n",
    "        except (AttributeError, IndexError):\n",
    "            rooms = 'N/A'\n",
    "\n",
    "        properties.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Rooms': rooms,\n",
    "            'URL': url\n",
    "        })\n",
    "        \n",
    "    return properties\n",
    "\n",
    "# URL template with pagination\n",
    "url_template = 'https://www.imcongo.com/congo-a-louer-tri--mode-list-recherche--congo-2-immo-en.html?page={}'\n",
    "\n",
    "# Number of pages to crawl (adjust this based on how many pages are available)\n",
    "num_pages = 5  # Adjust this number based on actual pagination\n",
    "\n",
    "# List to store all scraped properties\n",
    "all_properties = []\n",
    "\n",
    "# File to save the progress incrementally\n",
    "output_file = 'imcongo_rent_listings.csv'\n",
    "\n",
    "# Check if the file already exists, so we don't overwrite data during scraping\n",
    "if os.path.exists(output_file):\n",
    "    existing_df = pd.read_csv(output_file)\n",
    "    all_properties = existing_df.to_dict('records')  # Load existing data into list\n",
    "else:\n",
    "    existing_df = pd.DataFrame()  # Create empty DataFrame if file doesn't exist\n",
    "\n",
    "# Crawl through multiple pages\n",
    "for page_num in range(1, num_pages + 1):\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    \n",
    "    # Generate the URL for the current page\n",
    "    url = url_template.format(page_num)\n",
    "    \n",
    "    # Send a GET request to fetch the HTML content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract property details from the page\n",
    "        properties = extract_property_details(soup)\n",
    "        \n",
    "        # If no properties found, break (assumes no listings means end of pages)\n",
    "        if not properties:\n",
    "            print(f\"No more listings found on page {page_num}. Stopping.\")\n",
    "            break\n",
    "        \n",
    "        # Add the scraped data to the list\n",
    "        all_properties.extend(properties)\n",
    "        \n",
    "        # Save progress after each page\n",
    "        df = pd.DataFrame(all_properties)\n",
    "        df.to_csv(output_file, index=False)  # Save to CSV after scraping each page\n",
    "        \n",
    "        # Delay to avoid overloading the server\n",
    "        time.sleep(2)  # Sleep for 2 seconds between requests\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_num}, status code: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "print(f\"Scraping complete. Data saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EGYPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zn/11vhd4q978d_75hwl7qlbcjw0000gp/T/ipykernel_1460/1302806940.py:30: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  bedroom_td = listing.find('td', text=lambda x: x and 'Beds' in x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(output_file, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Save to CSV after scraping each page\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# Delay to avoid overloading the server\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Sleep for 2 seconds between requests\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Function to extract property details\n",
    "def extract_property_details(soup):\n",
    "    properties = []\n",
    "    \n",
    "    # Find all property listings\n",
    "    listings = soup.find_all('div', class_='propertyItem')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Extract title (from <a> tag with class 'propertyImgLink')\n",
    "            title_tag = listing.find('a', class_='propertyImgLink')\n",
    "            title = title_tag['title'].strip() if title_tag else 'N/A'\n",
    "        except AttributeError:\n",
    "            title = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            # Extract price (from <span> tag with class 'price')\n",
    "            price_tag = listing.find('span', class_='price')\n",
    "            price = price_tag.text.strip() if price_tag else 'N/A'\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract bedroom info (from the <td> containing \"Beds\" text)\n",
    "            bedroom_td = listing.find('td', text=lambda x: x and 'Beds' in x)\n",
    "            bedroom_info = bedroom_td.text.strip() if bedroom_td else 'N/A'\n",
    "        except AttributeError:\n",
    "            bedroom_info = 'N/A'\n",
    "        \n",
    "        properties.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Bedrooms': bedroom_info\n",
    "        })\n",
    "        \n",
    "    return properties\n",
    "\n",
    "# URL template with pagination\n",
    "url_template = 'http://www.theegyptrealestate.com/properties/all/date_desc/grid/{}'\n",
    "\n",
    "# Number of pages to crawl (adjust based on the actual last page)\n",
    "num_pages = 221  # Change this if needed\n",
    "\n",
    "# List to store all scraped properties\n",
    "all_properties = []\n",
    "\n",
    "# File to save the progress incrementally\n",
    "output_file = 'egypt_rent_listings.csv'\n",
    "\n",
    "# Crawl through multiple pages\n",
    "for page_num in range(1, num_pages + 1):\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    \n",
    "    # Generate the URL for the current page\n",
    "    url = url_template.format(page_num)\n",
    "    \n",
    "    # Send a GET request to fetch the HTML content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract property details from the page\n",
    "        properties = extract_property_details(soup)\n",
    "        \n",
    "        # If no properties found, break (assumes no listings means end of pages)\n",
    "        if not properties:\n",
    "            print(f\"No more listings found on page {page_num}. Stopping.\")\n",
    "            break\n",
    "        \n",
    "        # Add the scraped data to the list\n",
    "        all_properties.extend(properties)\n",
    "        \n",
    "        # Save progress after each page\n",
    "        df = pd.DataFrame(all_properties)\n",
    "        df.to_csv(output_file, index=False)  # Save to CSV after scraping each page\n",
    "        \n",
    "        # Delay to avoid overloading the server\n",
    "        time.sleep(2)  # Sleep for 2 seconds between requests\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_num}, status code: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "print(f\"Scraping complete. Data saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zn/11vhd4q978d_75hwl7qlbcjw0000gp/T/ipykernel_1460/2846833242.py:14: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  bed_td = listing.find('td', text=False)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to extract bedroom information\n",
    "def extract_bedroom_info(soup):\n",
    "    bedrooms = []\n",
    "    \n",
    "    # Find all property listings\n",
    "    listings = soup.find_all('div', class_='propertyItem')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Find the <td> that contains the bed icon and extract the number of beds\n",
    "            bed_td = listing.find('td', text=False)\n",
    "            bed_img = bed_td.find('img', alt=True)\n",
    "            \n",
    "            if bed_img and 'Beds' in bed_img['alt']:\n",
    "                # Extract the number of beds from the alt attribute\n",
    "                bed_info = bed_img['alt'].strip()\n",
    "            else:\n",
    "                bed_info = 'N/A'\n",
    "        except AttributeError:\n",
    "            bed_info = 'N/A'\n",
    "        \n",
    "        bedrooms.append(bed_info)\n",
    "    \n",
    "    return bedrooms\n",
    "\n",
    "# URL to scrape (example)\n",
    "url = 'http://www.theegyptrealestate.com/properties/all/date_desc/grid/1'\n",
    "\n",
    "# Send a GET request to fetch the HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract bedroom information\n",
    "    bedroom_info = extract_bedroom_info(soup)\n",
    "    \n",
    "    # Print the extracted bedroom info\n",
    "    print(bedroom_info)\n",
    "else:\n",
    "    print(f\"Failed to fetch the page, status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m url \u001b[38;5;241m=\u001b[39m url_template\u001b[38;5;241m.\u001b[39mformat(page_num)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Send a GET request to fetch the HTML content\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Check if the request was successful\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# Parse the HTML content using BeautifulSoup\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Function to extract property details\n",
    "def extract_property_details(soup):\n",
    "    properties = []\n",
    "    \n",
    "    # Find all property listings\n",
    "    listings = soup.find_all('div', class_='propertyItem')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Extract title (from <a> tag with class 'propertyImgLink')\n",
    "            title_tag = listing.find('a', class_='propertyImgLink')\n",
    "            title = title_tag['title'].strip() if title_tag else 'N/A'\n",
    "        except AttributeError:\n",
    "            title = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            # Extract price (from <span> tag with class 'price')\n",
    "            price_tag = listing.find('span', class_='price')\n",
    "            price = price_tag.text.strip() if price_tag else 'N/A'\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract bedroom info from the <td> containing the <img> tag with alt attribute\n",
    "            bedroom_info_tag = listing.find('td').find('img', alt=True)\n",
    "            bedroom_info = bedroom_info_tag['alt'].strip() if bedroom_info_tag else 'N/A'\n",
    "        except AttributeError:\n",
    "            bedroom_info = 'N/A'\n",
    "        \n",
    "        properties.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Bedrooms': bedroom_info\n",
    "        })\n",
    "        \n",
    "    return properties\n",
    "\n",
    "# URL template with pagination\n",
    "url_template = 'http://www.theegyptrealestate.com/properties/all/date_desc/grid/{}'\n",
    "\n",
    "# Number of pages to crawl (adjust based on the actual last page)\n",
    "num_pages = 221  # Change this if needed\n",
    "\n",
    "# List to store all scraped properties\n",
    "all_properties = []\n",
    "\n",
    "# File to save the progress incrementally\n",
    "output_file = 'egypt_rent_listings.csv'\n",
    "\n",
    "# Crawl through multiple pages\n",
    "for page_num in range(1, num_pages + 1):\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    \n",
    "    # Generate the URL for the current page\n",
    "    url = url_template.format(page_num)\n",
    "    \n",
    "    # Send a GET request to fetch the HTML content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract property details from the page\n",
    "        properties = extract_property_details(soup)\n",
    "        \n",
    "        # If no properties found, break (assumes no listings means end of pages)\n",
    "        if not properties:\n",
    "            print(f\"No more listings found on page {page_num}. Stopping.\")\n",
    "            break\n",
    "        \n",
    "        # Add the scraped data to the list\n",
    "        all_properties.extend(properties)\n",
    "        \n",
    "        # Save progress after each page\n",
    "        df = pd.DataFrame(all_properties)\n",
    "        df.to_csv(output_file, index=False)  # Save to CSV after scraping each page\n",
    "        \n",
    "        # Delay to avoid overloading the server\n",
    "        time.sleep(2)  # Sleep for 2 seconds between requests\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_num}, status code: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "print(f\"Scraping complete. Data saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
