{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page fetched successfully!\n",
      "Data saved to 'propertypro_listings.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract property details\n",
    "def extract_property_details(soup):\n",
    "    properties = []\n",
    "    \n",
    "    # Find all property listings\n",
    "    listings = soup.find_all('div', class_='property-listing-grid')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Extract price\n",
    "            price = listing.find('div', class_='pl-price').find('h3').text.strip()\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            # Extract title\n",
    "            title = listing.find('div', class_='pl-title').text.strip()\n",
    "        except AttributeError:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract property ID\n",
    "            pid = listing.find('p').text.strip().replace(\"PID :\", \"\")\n",
    "        except AttributeError:\n",
    "            pid = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract number of beds and baths\n",
    "            details = listing.find('h6').text.strip()\n",
    "        except AttributeError:\n",
    "            details = 'N/A'\n",
    "        \n",
    "        properties.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'PID': pid,\n",
    "            'Details': details\n",
    "        })\n",
    "        \n",
    "    return properties\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = 'https://www.propertypro.ng/property-for-sale'\n",
    "\n",
    "# Send a GET request to fetch the HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Page fetched successfully!\")\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract property details\n",
    "    properties = extract_property_details(soup)\n",
    "    \n",
    "    # Convert the data into a DataFrame\n",
    "    df = pd.DataFrame(properties)\n",
    "    \n",
    "    # Save the data into a CSV file\n",
    "    df.to_csv('propertypro_listings.csv', index=False)\n",
    "    print(\"Data saved to 'propertypro_listings.csv'\")\n",
    "else:\n",
    "    print(f\"Failed to fetch page, status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Data saved to 'propertypro_listings_all_pages.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Function to extract property details\n",
    "def extract_property_details(soup):\n",
    "    properties = []\n",
    "    \n",
    "    # Find all property listings\n",
    "    listings = soup.find_all('div', class_='property-listing-grid')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Extract price\n",
    "            price = listing.find('div', class_='pl-price').find('h3').text.strip()\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            # Extract title\n",
    "            title = listing.find('div', class_='pl-title').text.strip()\n",
    "        except AttributeError:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract property ID\n",
    "            pid = listing.find('p').text.strip().replace(\"PID :\", \"\")\n",
    "        except AttributeError:\n",
    "            pid = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract number of beds and baths\n",
    "            details = listing.find('h6').text.strip()\n",
    "        except AttributeError:\n",
    "            details = 'N/A'\n",
    "        \n",
    "        properties.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'PID': pid,\n",
    "            'Details': details\n",
    "        })\n",
    "        \n",
    "    return properties\n",
    "\n",
    "# URL template (with page number placeholder)\n",
    "url_template = 'https://www.propertypro.ng/property-for-sale?page={}'\n",
    "\n",
    "# Number of pages to crawl\n",
    "num_pages = 10  # Adjust this based on how many pages you want to scrape\n",
    "\n",
    "# List to store all scraped properties\n",
    "all_properties = []\n",
    "\n",
    "# Crawl through multiple pages\n",
    "for page_num in range(1, num_pages + 1):\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    # Generate the URL for the current page\n",
    "    url = url_template.format(page_num)\n",
    "    \n",
    "    # Send a GET request to fetch the HTML content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract property details from the page\n",
    "        properties = extract_property_details(soup)\n",
    "        \n",
    "        # Add the scraped data to the list\n",
    "        all_properties.extend(properties)\n",
    "        \n",
    "        # Delay to avoid overloading the server\n",
    "        time.sleep(2)  # Sleep for 2 seconds between requests\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_num}, status code: {response.status_code}\")\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(all_properties)\n",
    "\n",
    "# Save the data into a CSV file\n",
    "df.to_csv('propertypro_listings_all_pages.csv', index=False)\n",
    "print(\"Data saved to 'propertypro_listings_all_pages.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 for rent listings...\n",
      "Scraping page 2 for rent listings...\n",
      "Scraping page 3 for rent listings...\n",
      "Scraping page 4 for rent listings...\n",
      "Scraping page 5 for rent listings...\n",
      "Scraping page 6 for rent listings...\n",
      "Scraping page 7 for rent listings...\n",
      "Scraping page 8 for rent listings...\n",
      "Scraping page 9 for rent listings...\n",
      "Scraping page 10 for rent listings...\n",
      "Data saved to 'propertypro_rent_listings.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Function to extract property details for rent listings\n",
    "def extract_rental_details(soup):\n",
    "    properties = []\n",
    "    \n",
    "    # Find all property listings for rent\n",
    "    listings = soup.find_all('div', class_='property-listing-grid')\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Extract price\n",
    "            price = listing.find('div', class_='pl-price').find('h3').text.strip()\n",
    "        except AttributeError:\n",
    "            price = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            # Extract title\n",
    "            title = listing.find('div', class_='pl-title').text.strip()\n",
    "        except AttributeError:\n",
    "            title = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract property ID\n",
    "            pid = listing.find('p').text.strip().replace(\"PID :\", \"\")\n",
    "        except AttributeError:\n",
    "            pid = 'N/A'\n",
    "\n",
    "        try:\n",
    "            # Extract number of beds and baths\n",
    "            details = listing.find('h6').text.strip()\n",
    "        except AttributeError:\n",
    "            details = 'N/A'\n",
    "        \n",
    "        properties.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'PID': pid,\n",
    "            'Details': details\n",
    "        })\n",
    "        \n",
    "    return properties\n",
    "\n",
    "# URL template for houses for rent (with page number placeholder)\n",
    "url_template = 'https://www.propertypro.ng/property-for-rent?page={}'\n",
    "\n",
    "# Number of pages to crawl (you can adjust this)\n",
    "num_pages = 10\n",
    "\n",
    "# List to store all scraped properties\n",
    "all_rental_properties = []\n",
    "\n",
    "# Crawl through multiple pages for rent listings\n",
    "for page_num in range(1, num_pages + 1):\n",
    "    print(f\"Scraping page {page_num} for rent listings...\")\n",
    "    url = url_template.format(page_num)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        rental_properties = extract_rental_details(soup)\n",
    "        all_rental_properties.extend(rental_properties)\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_num}, status code: {response.status_code}\")\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df_rentals = pd.DataFrame(all_rental_properties)\n",
    "\n",
    "# Save the data into a CSV file\n",
    "df_rentals.to_csv('propertypro_rent_listings.csv', index=False)\n",
    "print(\"Data saved to 'propertypro_rent_listings.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
